{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffaf97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "s\n",
    "wv = KeyedVectors.load_word2vec_format(\"/kaggle/input/google-word2vec/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "def explore(w1, w2, w3):\n",
    "    vec = wv[w1] - wv[w2] + wv[w3]\n",
    "    return [(w, s) for w, s in wv.similar_by_vector(vec, topn=10) if w not in {w1, w2, w3}]\n",
    "\n",
    "def visualize(words, vecs, method='pca'):\n",
    "    reducer = TSNE(n_components=2, random_state=42, perplexity=min(len(words) - 1, 30)) if method == 'tsne' else PCA(n_components=2)\n",
    "    coords = reducer.fit_transform(vecs)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for w, (x, y) in zip(words, coords):\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.02, y + 0.02, w)\n",
    "    plt.title(f\"Word Embeddings Visualization using {method.upper()}\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "base = [\"king\", \"man\", \"woman\", \"queen\", \"prince\", \"princess\", \"royal\", \"throne\"]\n",
    "related = explore(\"king\", \"man\", \"woman\")\n",
    "all_words = base + [w for w, _ in related]\n",
    "vecs = np.array([wv[w] for w in all_words if w in wv])\n",
    "\n",
    "visualize(all_words, vecs, 'pca')\n",
    "visualize(all_words, vecs, 'tsne')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(\"/kaggle/input/google-word2vec/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "def visualize(words, method='pca', perplexity=30):\n",
    "    vecs = np.array([wv[w] for w in words if w in wv])\n",
    "    coords = (PCA(n_components=2) if method == 'pca' else TSNE(n_components=2, random_state=42, perplexity=perplexity)).fit_transform(vecs)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for w, (x, y) in zip(words, coords):\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.02, y + 0.02, w)\n",
    "    plt.title(f\"{method.upper()} Visualization\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "words = [\"computer\", \"software\", \"hardware\", \"algorithm\", \"data\", \"network\", \"programming\", \"machine\", \"learning\", \"artificial\"]\n",
    "visualize(words, method='pca')\n",
    "visualize(words, method='tsne', perplexity=3)\n",
    "\n",
    "for word in [\"computer\", \"learning\"]:\n",
    "    print(f\"\\nTop 5 similar words to '{word}':\")\n",
    "    for w, s in wv.most_similar(word, topn=5)\n",
    "  print(f\"{w}: {s:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
